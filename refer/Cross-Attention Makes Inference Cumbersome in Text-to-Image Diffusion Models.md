# Cross-Attention Makes Inference Cumbersome in Text-to-Image Diffusion Models

Wentian Zhang* Haozhe Liu<sup>1</sup>* Jinheng Xie<sup>2</sup>* Francesco Faccio<sup>1,3</sup>  Mike Zheng Shou<sup>2</sup> JÃ¼rgen Schmidhuber<sup>1,3</sup>

![](images/d251a3b720f6c00783eadeca0d19a1ffbdc5cb3111caed49956457874c73ffdb.jpg)  
Figure 1: This study reveals that, in text-to-image diffusion models, cross-attention is crucial only in the early inference steps, allowing us to cache and reuse the cross-attention map in later steps. This yields a considerable acceleration for inference, especially for the model with high-resolution input: (a) SD-XL (Podell et al., 2023) and (b) PixArt-Alpha (Chen et al., 2023a). The proposed method, namely TGATE, is highlighted by its broad applications in (c). MACs refer to Multiple-Accumulate Operations. For LCM (Luo et al., 2023), the total number of inference steps is distilled to 4, but TGATE can still further accelerate its denoising process.

# Abstract

This study explores the role of cross- attention during inference in text- conditional diffusion models. We find that cross- attention outputs converge to a fixed point after few inference steps. Accordingly, the time point of convergence naturally divides the entire inference process into two stages: an initial semantics- planning stage, during which, the model relies on cross- attention to plan text- oriented visual semantics, and a subsequent fidelity- improving stage, during which the model tries to generate images from previously planned semantics. Surprisingly, ignoring text conditions in the fidelity- improving stage not only reduces computation complexity, but also maintains model performance. This yields a simple and training- free method called TGATE for efficient generation, which caches the cross- attention output once it converges and keeps it fixed during the remaining inference steps. Our empirical study on the MS- COCO validation set confirms its effectiveness. The source code of TGATE is available at https://github.com/HaozheLiu- ST/T- GATE.

# 1.Introduction

A small leak will sink a great ship.

Benjamin Franklin

Diffusion models (Jarzynski, 1997; Neal, 2001; Ho et al., 2020) have been widely used for image generation. Featuring cross- attention (Voswani et al., 2017), they align different modalities (Rombach et al., 2022), including text, for conditional generation tasks. Several studies highlight the importance of cross- attention for spatial control (Xie et al., 2023; Hertz et al., 2023; Chefer et al., 2023), but few, if any, investigate its role from a temporal perspective during the denoising process.

Here, we address a new question: "is cross- attention essential for each step during the inference of text- to- image diffusion models?" To this end, we study the influence of cross- attention on the quality of generated images at each inference step. Our findings highlight two counter- intuitive points:

Cross- attention outputs converge to a fixed point in the first few steps. Accordingly, the time point of convergence divides the denoising process of the diffusion models into two stages: i) an initial stage, during which the model relies on cross- attention to plan text- oriented visual semantics; we denote it as semantics- planning stage, and ii) a subsequent stage, during which the model learns to generate images from previous semantic planning; we refer to it as fidelityimproving stage.

Cross- Attention is redundant in the fidelityimproving stage. During the semantics- planning stage, cross- attention plays a crucial role in creating meaningful semantics. Yet, in the latter stage, cross- attention converges and has a minor impact on the generation process. Indeed, by passing cross- attention during the fidelity- improving stage can potentially reduce the computation cost while maintaining the image generation quality.

Notably, the scaled dot product in cross- attention is an operation with quadratic complexity. With the continuously increasing resolution and token length in modern models, cross- attention inevitably causes expensive computation, becoming a significant source of latency in applications such as mobile devices (Li et al., 2024). This insight prompts a re- evaluation of cross- attention's role and has inspired us to design a simple, effective, and training- free method, i.e., temporally gating the cross- attention (TGATE), to improve the efficiency and maintain the generation quality of offthe- shelf diffusion models. Our principle observations with respect to TGATE are as follows:

TGATE increases efficiency by caching and reusing the outcomes of cross- attentions after its convergence, thereby eliminating redundant cross- attentions in the fidelity- improving stage. TGATE does not result in a performance drop, as the results of cross- attention are converged and redundant. In fact, we observe a slight improvement in the Frechet Inception Distance (FID) over the baseline. TGATE can reduce 65T Multiple- Accumulate Operations (MACs) per image and eliminate O.5B parameters in the fidelity- improving stage, resulting in a reduction of around  $50\%$  in latency compared to the baseline model (SDXL) without training costs.

# 2.Related Works

This study analyzes the role and functionality of crossattention within diffusion trajectories. Not surprisingly, this topic has been explored from various angles in prior research. Spectral Diffusion (Yang et al., 2023) traces diffusion trajectory through frequency analysis and finds that the diffusion model, at each step, restores the image from varying frequency components. T- stitch (Pan et al., 2024) shows that, at the beginning of the inference process, the noise generated by different models is similar. This finding suggests that a smaller model can produce the same noise as a larger one, greatly reducing computational costs. Adaptive Guidance (Castillo et al., 2023) models the diffusion trajectory as a graph and applies neural architecture search (NAS) to automatically identify the importance of each step. This approach has identified Classifier- free Guidance (CFG) (Ho & Salimans, 2022) as a redundant operator in some inference steps, suggesting to remove unconditional batch for accelerating the generation speed. DeepCache (Ma et al., 2023) illustrates that there are temporal similarities in the predictions made by each block in consecutive time steps. Consequently, reutilizing predictions from these blocks can improve the efficiency of the inference process. Wimbauer et al. (Wimbauer et al., 2023) propose a contemporary work for caching block features, but require a resource- friendly training process.

To the best of our knowledge, our study is orthogonal to existing studies in the community: we discover that text embedding does not consistently benefit the entire inference process. Therefore, cross- attention outcomes can be selectively copied and reused in certain inference steps without affecting generation performance. This may inspire several new research lines toward an efficient diffusion process.

# 3. Preliminary

Diffusion technology has a rich history from 1997, dating back to non- equilibrium statistical physics (Jarzynski, 1997)

and annealed importance sampling (Neal, 2001). This mechanism, featured by its scalability and stability (Dhariwal & Nichol, 2021; Ho et al., 2020), has been widely used in modern text- conditional generative models (Rombach et al., 2022; Ramesh et al., 2022; 2021; Saharia et al., 2022; Chen et al., 2023b; Brooks et al.).

Learning Objective. In this paper, we adopt the formulation introduced by Rombach et al. (2022) to construct a latent diffusion model comprising four main components: an image encoder  $\mathcal{E}(\cdot)$ , an image decoder  $\mathcal{D}(\cdot)$ , a denoising U- Net  $\epsilon_{\theta}(\cdot)$ , and a text embedding  $c$ . The learning objective for this model is defined as follows:

$$
\mathcal{L}_{\theta} = \mathbb{E}_{z_0\sim \mathcal{E}(x),t,c,\epsilon \sim \mathcal{N}(0,1)}\left[||\epsilon -\epsilon_{\theta}(z_t,t,c)||_2^2\right], \tag{1}
$$

where  $\epsilon_{\theta}(\cdot)$  is designed to accurately estimate the noise  $\epsilon$  added to the current latent representation  $z_{t}$  at time step  $t\in [1,n]$ , conditioned on the text embedding  $c$ . During inference,  $\epsilon_{\theta}(z_t,t,c)$  is called multiple times to recover  $z_{0}$  from  $z_{t}$ , where  $z_{0}$  is decoded into an image  $x$  through  $\mathcal{D}(z_0)$ .

Inference Stage. In inference, classifier- free guidance (CFG) (Ho & Salimans, 2022) is commonly employed to incorporate conditional guidance:

$$
\begin{array}{rl} & {\epsilon_{c,\theta}(z_t,t,w,c) =}\\ & {\epsilon_\theta (z_t,t,\emptyset) + w(\epsilon_\theta (z_t,t,c) - \epsilon_\theta (z_t,t,\emptyset)),} \end{array} \tag{2}
$$

where  $\varnothing$  represents the embedding of null text, i.e.,  $w$  is the guidance scale parameter, and  $\epsilon_{c,\theta}$  implicitly estimates  $p(c|z)\propto p(z|c) / p(z)$  to guide conditional generation  $\tilde{p} (z|c)\propto p(z|c)p^{w}(c|z)$ . In particular,  $\nabla log(p(c|z))\propto \nabla_{z}log(p(z|c)) - \nabla_{z}log(p(z))$ , which is identical to Eq. 2.

Cross- Attention. Beyond the vision domain, the cross- attention module (Vaswani et al., 2017) is integrated into each block of the denoising U- Net,  $\epsilon_{\theta}$ , facilitating interaction with various input modalities, including text. It is mathematically represented as:

$$
\mathbf{C}_c^t = \mathrm{Softmax}\left(\frac{Q_z^t\cdot K_c}{\sqrt{d}}\right)\cdot V_c, \tag{3}
$$

where  $Q_{z}^{t}$  is a projection of  $z_{t}$ ,  $K_{c}$  and  $V_{c}$  are different projections of the text embedding  $c$ , and  $d$  is the feature dimension of  $K_{c}$ . The cross- attention mechanism allows text prompts to guide the generation process. However, due to its quadratic computational complexity, cross- attention is a significant bottleneck when processing high- resolution features (Li et al., 2024).

# 4. Temporal Analysis of Cross-Attention

Here, we unveil the role and functionality of cross- attention in the inference steps of a well- trained diffusion model. We start by empirically demonstrating an observation of cross- attention map convergence in Section 4.1, then presenting a systematic analysis of the observation in Section 4.2.

![](images/dc60ff7b244ede33e78bdc94c77e94b8da57af277331f03e2b58f6ec5a2ec987.jpg)  
Figure 2: Illustration of the difference of cross-attention maps between two consecutive inference steps on MS-COCO dataset. Each data point in the figure is the average of 1,000 captions and all cross-attention maps within the model. The shaded area indicates the variance, while the curve demonstrates that the difference between consecutive steps progressively approaches zero.

# 4.1. On the Convergence of Cross-Attention Map

In diffusion models, cross- attention mechanisms furnish textual guidance at each step. However, given the shifts in noise input across these steps, it prompts thinking: Do the feature maps generated by cross- attention exhibit temporal stability or fluctuate over time?

To find an answer, we randomly collect 1,000 captions from the MS- COCO dataset and generate images using a pre- trained SD- 2.1 model with CFG. During inference, we calculate the L2 distance between  $\mathbf{C}^t$  and  $\mathbf{C}^{t + 1}$ , where  $\mathbf{C}^t$  represents the cross- attention maps at time step  $t$ . The difference in cross- attention between the two steps is obtained by averaging L2 distances among all input captions, conditions, and depths.

Fig. 2 illustrates the variation of cross- attention differences across various inference steps. A clear trend emerges, showing a gradual convergence of the differences towards zero. The convergence always appears within 5 to 10 inference steps. Therefore, Cross- Attention maps converge to a fixed point and no longer offer dynamic guidance for image generation. This finding supports the effectiveness of CFG from the perspective of cross- attention, demonstrating that despite varying conditions and initial noise, unconditional and conditional batches can converge toward a single consistent result (Castillo et al., 2023).

![](images/1eadc7019ea88e4c8f6962619909f99edb4d499de1ae51d4491a0b02474430cb.jpg)  
Figure 3: Illustration of the impact of cross-attention on the inference steps in a pre-trained diffusion model, i.e., stable diffusion 2.1 (SD-2.1). (a) The mean of the noise predicted at each inference step. (b) Images generated by the diffusion model at different inference steps. The first row,  $\mathbf{S}$ , in (b) feeds the text embedding to the cross-attention modules for all steps, the second row,  $\mathbf{S}_m^F$ , only uses the text embedding from the first step to the  $m$ -th step, and the third row,  $\mathbf{S}_m^L$ , inputs the text embedding from the  $m$ -th to the  $n$ -th step. (c) Zero-shot FID scores based on these three settings on the MS-COCO validation set (Lin et al., 2014), with the baseline defined as conditional generation without CFG.

This phenomenon shows that the impact of cross- attention in the inference process is not uniform and inspires the temporal analysis of cross- attention in the next section.

# 4.2. The Role of Cross-Attention in Inference

Analytical Tool. Existing analysis (Ma et al., 2023) shows that the consecutive inference steps of diffusion models share similar denoising behaviors. Consequently, inspired by behavioral explanation (Bau et al., 2020; Liu et al., 2023), we measure the impact of cross- attention by effectively "removing" it at a specific stage and observing the resulting difference in generation quality. In practice, this removal is approximated by substituting the original text embedding with a placeholder for null text, i.e., "". We formalize the standard denoising trajectory as a sequence:

$$
\mathbf{S} = \{\epsilon_{\mathrm{c}}(z_{n},c),\epsilon_{\mathrm{c}}(z_{m - 1},c),\dots,\epsilon_{\mathrm{c}}(z_{1},c)\} , \tag{4}
$$

where we omit the time step  $t$  and guidance scale  $w$  for simplicity. The image generated from the sequence  $\mathbf{S}$  is denoted by  $x$ . We then modify this standard sequence by replacing the conditional text embedding  $c$  with the null text embedding  $\mathcal{Q}$  over a specified inference interval, resulting in two new sequences,  $\mathbf{S}_m^F$  and  $\mathbf{S}_m^L$ , based on a scalar  $m$  as

$$
\begin{array}{r}\mathbf{S}_m^{\mathrm{F}} = \{\epsilon_{\mathrm{c}}(z_n,c),\dots ,\epsilon_{\mathrm{c}}(z_m,c),\dots ,\epsilon_{\mathrm{c}}(z_1,\emptyset)\} ,\\ \mathbf{S}_m^{\mathrm{L}} = \{\epsilon_{\mathrm{c}}(z_n,\emptyset),\dots ,\epsilon_{\mathrm{c}}(z_m,\emptyset),\dots ,\epsilon_{\mathrm{c}}(z_1,c)\} . \end{array} \tag{5}
$$

Here,  $m$  serves as a gate step that splits the trajectory into two stages. In the sequence  $\mathbf{S}_m^F$ , the null text embedding  $\mathcal{Q}$  replaces the original text embedding  $c$  for the steps from  $m + 1$  to  $n$ . On the contrary, in the sequence  $\mathbf{S}_m^L$ , the steps from 1 to  $m$  utilize the null text embedding  $\mathcal{Q}$  instead of the original text embedding  $c$ , while the steps from  $m$  to  $n$  continue to use the original text embedding  $c$ . We denote the images generated from these two trajectories as  $x_{m}^{\mathrm{F}}$  and  $x_{m}^{\mathrm{L}}$ , respectively. To determine the impact of cross- attention at different stages, we compare the differences in generation quality among  $x$ ,  $x_{m}^{\mathrm{L}}$ , and  $x_{m}^{\mathrm{M}}$ . If there is a significant difference in generation quality among  $x$  and  $x_{m}^{\mathrm{F}}$ , it indicates the importance of cross- attention at that stage. On the contrary, if there is no substantial variation, the inclusion of cross- attention may not be necessary.

We use SD- 2.1 as our base model and employ the DPM solver (Lu et al., 2022) for noise scheduling. The inference step in all experiments is set as 25. The text prompt "High quality photo of an astronaut riding a horse in space." is used for visualization.

Results and Discussions. We provide the trajectory of the mean of predicted noise in Fig. 3(a), which empirically shows that the denoising process converges after 25 inference steps. Therefore, it is sufficient to analyze the impact of cross- attention within this interval. As shown in Fig. 3(b), we set the gate step  $m$  to 10, which yields three trajectories:  $\mathbf{S}$ ,  $\mathbf{S}_m^F$  and  $\mathbf{S}_m^L$ . The visualization illustrates that ignoring

Table 1: Zero-shot FIDs from MS-COCO validation set using the base model SD-2.1 with DPM-Solver (Lu et al., 2022).  $m$  is the gate step.  

<table><tr><td>Inference Method</td><td>FID â</td></tr><tr><td>SD-2.1(w/CFG)</td><td>22.609</td></tr><tr><td>SMF(m=3)</td><td>29.282</td></tr><tr><td>SMF(m=5)</td><td>20.859</td></tr><tr><td>SMF(m=10)</td><td>21.816</td></tr></table>

Table 2: Zero-shot FIDs on MS-COCO validation set using the base model SD-2.1 with DPM-Solver (Lu et al., 2022).  $m$  is the gate step and  $n$  is the total inference number.  

<table><tr><td>Configuration</td><td>S</td><td>SMF</td><td>Slm</td></tr><tr><td>n=15, m=6</td><td>23.380</td><td>22.315</td><td>58.580</td></tr><tr><td>n=25, m=10</td><td>22.609</td><td>21.816</td><td>53.265</td></tr><tr><td>n=50, m=20</td><td>22.445</td><td>21.557</td><td>48.376</td></tr><tr><td>n=100, m=25</td><td>22.195</td><td>20.391</td><td>26.413</td></tr></table>

the cross- attention after 10 steps does not influence the ultimate outcomes. However, bypassing cross- attention in the initial steps results in a notable disparity. As shown in Fig. 3(c), this elimination incurs a significant drop in generation quality (FID) in the MS- COCO validation set, which is even worse than the weak baseline that generates images without CFG. We perform further experiments for different gate steps of  $\{3,5,10\}$ . As shown in Table 1, when the gate step is larger than five steps, the model that ignores cross- attention can achieve better FIDs. To further justify the generalization of our findings, we conduct experiments under various conditions, including a range of total inference numbers, noise schedulers, and base models. As shown in Table 2, 3 and 4, we report the FIDs of S,  $\mathbf{S}_m^{\mathrm{F}}$  and  $\mathbf{S}_m^{\mathrm{L}}$  on MS- COCO validation set. The experimental results consistently show the FIDs of  $\mathbf{S}_m^{\mathrm{F}}$  are slightly better than the baseline S and outperform  $\mathbf{S}_m^{\mathrm{L}}$  by a wide margin. These studies underscore the potential of the findings for broad applicability.

We summarize our analyses as follows:

Cross- attention converges early during the inference process, which can be characterized by semantics- planning and fidelity- improving stages. The impact

Table 3: Zero-shot FIDs on MS-COCO validation set using the base model SD-2.1 with different noise schedulers. The total inference number is set as 50, and the gate step is 20.  

<table><tr><td>Configuration</td><td>S</td><td>SMF</td><td>Slm</td></tr><tr><td>EulerD (Karras et al., 2022)</td><td>22.507</td><td>21.559</td><td>47.108</td></tr><tr><td>DPMSolver (Lu et al., 2022)</td><td>22.445</td><td>21.557</td><td>48.376</td></tr><tr><td>DDIM (Song et al., 2020)</td><td>21.235</td><td>20.495</td><td>53.150</td></tr></table>

Table 4: Zero-shot FIDs on MS-COCO validation set using different models: SD-1.5, SD-2,1, and SDXL. The total inference number is set as 25, and the gate step is 10.  

<table><tr><td>Configuration</td><td>S</td><td>SMF</td><td>Slm</td></tr><tr><td>SD-1.5 (Rombach et al., 2022)</td><td>23.927</td><td>22.974</td><td>37.271</td></tr><tr><td>SD-2.1 (Rombach et al., 2022)</td><td>22.609</td><td>21.816</td><td>53.265</td></tr><tr><td>SDXL (Podell et al., 2023)</td><td>24.628</td><td>23.195</td><td>108.676</td></tr></table>

of cross- attention is not uniform in these two stages.

Cross- attention in the semantics- planning stage is significant for generating the semantics aligned with the text conditions. The fidelity- improving stage mainly improves the image quality without the requirement of cross- attention. Null text embedding in this stage can slightly improve FID scores. Based on the above observations, we propose a method, dubbed TGATE, to remove cross- attention while saving computation and improving FID scores.

# 5. The Proposed Method - TGATE

Our empirical study has shown that cross- attention computation in the last inference steps is redundant. However, it is nontrivial to drop/replace cross- attention without retraining the model. Inspired by DeepCache (Ma et al., 2023), we propose an effective and training- free method named TGATE. This method caches the attention outcomes from the semantics- planning stage and reuses them throughout the fidelity- improving stage.

Caching Cross- Attention Maps. Suppose  $m$  is the gate step for stage transition. At the  $m$ - th step and for the  $i$ - th cross- attention module, two cross- attention maps,  $\mathbf{C}_c^{m,i}$  and  $\mathbf{C}_{\mathcal{O}}^{m,i}$ , can be accessed from CFG- based inference. We calculate the average of these two maps to serve as an anchor and store it in a First- In- First- Out feature cache  $\mathbf{F}$ . After traversing all the cross- attention blocks,  $\mathbf{F}$  can be written as:

$$
\mathbf{F} = \{\frac{1}{2} (\mathbf{C}_{\mathcal{O}}^{m,i} + \mathbf{C}_{c}^{m,i})|i\in [1,l]\} , \tag{6}
$$

where  $l$  represents the total number of cross- attention modules, with  $l = 16$  in SD- 2.1.

Re- using Cached Cross- Attention Maps. In each step of the fidelity- improving stage, when a cross- attention operation is encountered during the forward pass, it is omitted from the computation graph. Instead, the cached  $\mathbf{F}$ . pop(0) is fed into the subsequent computations. Note that this approach does not result in identical predictions at

each step, as the presence of a residual connection (Hochreiter, 1991; Srivastava et al., 2015; He et al., 2016) in U- Net (Ronneberger et al., 2015) allows the model to bypass cross- attention.

# 6. Experiment Results

# 6.1. Implementation Details

Base Models. We use four models: Stable Diffusion- 1.5 (SD- 1.5) (Rombach et al., 2022), SD- 2.1 (Rombach et al., 2022), SDXL (Podell et al., 2023), and PixArt- Alpha (Chen et al., 2023a), in our experiments. Among them, the SD series are based on convolutional neural networks (Fukushima, 1979; 1980; Zhang et al., 1988; LeCun et al., 1989; Hochreiter, 1991; Srivastava et al., 2015; He et al., 2016) (i.e., U- Net (Ronneberger et al., 2015)) and Pixart- Alpha works on the transformer (Vaswani et al., 2017) (i.e., DiT (Peebles & Xie, 2023)).

Acceleration Baselines. For a convincing empirical study, we compare our methods with several acceleration baseline methods: Latent Consistency Model (Luo et al., 2023), Adaptive Guidance (Castillo et al., 2023), DeepCache (Ma et al., 2023), and multiple noise schedulers (Karras et al., 2022; Lu et al., 2022; Song et al., 2020). Note that our method is orthogonal to existing methods for accelerating denoising inference; therefore, our method can be trivially integrated to further accelerate this process.

Evaluation Metrics. Akin to the previous studies (Podell et al., 2023), we use 10k images from the MS- COCO validation set (Lin et al., 2014) to evaluate the zero- shot generation performance. The images are generated using DPM- Solver (Lu et al., 2022) with a predefined 25 inference steps and resized to  $256 \times 256$  resolution to calculate FID (Heusel et al., 2017). To evaluate the efficiency, we use Calflops (xiaoju ye, 2023) to count Multiple- Accumulate Operations (MACs) and the number of Parameters (Params.). Furthermore, we assess the latency per sample on a platform equipped with a Nvidia 1080 Ti.

# 6.2. Improvement over Base Models

We integrate our method into a range of modern text conditional models (Rombach et al., 2022; Podell et al., 2023; Chen et al., 2023a). As shown in Table 5, in all settings, our method boosts the performance of the base models in computational efficiency, memory cost, and FID scores. In particular, our method works better when the number of parameters of the base model grows. In SDXL, TGATE can even reduce latency by half on the commercial gpu card (from 53.187s to 27.932s). Another feature of our method lies in Table 5: Computational complexity, memory cost, latency and FID on the MS- COCO validation set using the base model of SD- 1.5, SD- 2.1, SDXL, Pixart- Alpha. Params refers to number of parameters in the fidelity- improving stage, and MACs stands for Multiply- Accumulate Operations per image. These terms are automatically generated using Calflops. The latency of generating one image is tested on a 1080 Ti commercial card.

<table><tr><td>Inference Method</td><td>MACs</td><td>Params.</td><td>Latency</td><td>FID â</td></tr><tr><td>SD-1.5</td><td>16.938T</td><td>859.520M</td><td>7.032s</td><td>23.927</td></tr><tr><td>SD-1.5 + TGATE (m=5)</td><td>9.875T</td><td>815.557M</td><td>4.313s</td><td>20.789</td></tr><tr><td>SD-1.5 + TGATE (m=10)</td><td>11.641T</td><td>815.557M</td><td>4.993s</td><td>23.269</td></tr><tr><td>SD-2.1</td><td>38.041T</td><td>865.785M</td><td>16.121s</td><td>22.609</td></tr><tr><td>SD-2.1 + TGATE (m=5)</td><td>22.208T</td><td>815.433M</td><td>9.878s</td><td>19.940</td></tr><tr><td>SD-2.1 + TGATE (m=10)</td><td>26.166T</td><td></td><td>11.372s</td><td>21.294</td></tr><tr><td>SDXL</td><td>149.438T</td><td>2.370B</td><td>53.187s</td><td>24.628</td></tr><tr><td>SDXL + TGATE (m=5)</td><td>84.438T</td><td>2.024B</td><td>27.932s</td><td>22.738</td></tr><tr><td>SDXL + TGATE (m=10)</td><td>100.688T</td><td></td><td>34.246s</td><td>23.433</td></tr><tr><td>PixArt-Alpha</td><td>107.031T</td><td>611.350M</td><td>61.502s</td><td>38.669</td></tr><tr><td>PixArt-Alpha + TGATE (m=5)</td><td>57.956T</td><td></td><td>33.696s</td><td>42.663</td></tr><tr><td>PixArt-Alpha + TGATE (m=8)</td><td>65.318T</td><td>462.585M</td><td>37.867s</td><td>35.825</td></tr><tr><td>PixArt-Alpha + TGATE (m=10)</td><td>70.225T</td><td></td><td>40.648s</td><td>35.726</td></tr></table>

Table 6: Computational complexity, memory cost, latency and FID on the MS- COCO validation set using LCM distilled from SDXL and PixelArt- Alpha. The parameters (Params.) refers to the number of parameter in the fidelity- improving stage.

<table><tr><td>Inference Method</td><td>MACs</td><td>Params.</td><td>Latency</td><td>FID â</td></tr><tr><td>LCM (SDXL, n=4)</td><td>11.955T</td><td>2.570B</td><td>3.805s</td><td>25.044</td></tr><tr><td>LCM (SDXL, n=6)</td><td>17.933T</td><td>2.570B</td><td>5.708s</td><td>25.630</td></tr><tr><td>LCM (SDXL, n=8)</td><td>23.910T</td><td>2.570B</td><td>7.611s</td><td>27.413</td></tr><tr><td>+ TGATE (m=1)</td><td>11.171T</td><td>2.024B</td><td>3.533s</td><td>25.028</td></tr><tr><td>+ TGATE (m=2)</td><td>11.433T</td><td></td><td>3.624s</td><td>24.595</td></tr><tr><td>LCM (PixArt, n=4)</td><td>8.563T</td><td>611.350M</td><td>4.733s</td><td>36.086</td></tr><tr><td>+ TGATE (m=1)</td><td>7.623T</td><td>462.585M</td><td>4.448s</td><td>38.574</td></tr><tr><td>+ TGATE (m=2)</td><td>7.936T</td><td></td><td>4.543s</td><td>37.048</td></tr></table>

the friendly support for the transformer architecture. We integrate our method into PixArt- Alpha (Chen et al., 2023a), a test conditional model based on DiT (Peebles & Xie, 2023). Reusing cached cross- attention maps, TGATE can reduce the number of MACs from 107.031T to 65.318T, indicating the effectiveness and scalability of our method on the transformer architecture. Note that, based on our grid search, the cross- attention of PixArt- Alpha converges at around the eighth step. We argue that the improvement of efficiency comes from two aspects: i) As discussed previously, cross- attention is redundant during the fidelity- improving stage, and thus bypassing it can reduce the computation. ii) By caching and reusing cross- attention in the fidelity- improving stage, the conditional batch in CFG is accordingly removed from the computation graph to reduce computation costs.

Visualization. Additionally, we present qualitative compar-

![](images/2d405d04603c2669c747ccd32902f8b799d9705614344784d03cb074aeefec9f.jpg)  
Figure 4: Generated samples of (a) SDXL and (b) PixArt-Alpha with or without TGATE given the same initial noise and captions.

![](images/be85c15bb31b5f42f1920db60e1295ed9df76990663ecac77e741f17d81cd8b8.jpg)  
Figure 5: Generated samples of SD-2.1 (w/ CFG) and the proposed method TGATE with two gate steps (i.e., 5 and 10), given the same initial noises and captions.

isons between the proposed TGATE and its base models. Fig. 4 shows the images generated by different base models with or without TGATE. Fig. 5 presents the samples generated by different gate steps. These results are intuitive in that larger gate steps will lead to more similar generation results compared to base models without TGATE.

# 6.3. Improvement over Acceleration Models

Improvement over Consistency Model. Here, we implement our method with a distillation- based method (Schmidhuber, 1992b; Hinton et al., 2015), namely the latent consistency model (LCM). We first use the LCM distilled from  $\mathrm{SDXL}^3$  as the base model and perform a grid search for different inference steps. Table 6 reveals that fewer steps (set at four) improve generation performance. To incorporate

![](images/d1fd7b82d63c4ce2df0b51b85ba6e1ce5df56e79cf58d0f417ebb662212901ce.jpg)  
Figure 6: Generated samples of (a) LCM distilled from SDXL and (b) LCM with TGATE given the same initial noise and captions. (c) represents the difference between (a) and (b).

Table 7: Computational complexity, memory cost, latency, and FID on the MS-COCO validation set using DeepCache.  

<table><tr><td>Inference Method</td><td>MACs</td><td>Latency</td><td>FID â</td></tr><tr><td>SDXL</td><td>149.438T</td><td>53.187s</td><td>24.628</td></tr><tr><td>TGATE</td><td>84.438T</td><td>27.932s</td><td>22.738</td></tr><tr><td>DeepCache</td><td>57.888T</td><td>19.931s</td><td>23.755</td></tr><tr><td>DeepCache + TGATE</td><td>43.868T</td><td>14.666s</td><td>23.999</td></tr></table>

TGATE into LCM, we cache the attention prediction of the first or second step  $m = 1,2$  and reuse them in the remaining inference steps. Table 6 presents the experimental results of LCM models distilled from SDXL and PixArt- Alpha. Although the trajectory is deeply compressed into a few steps, our method still works well and can further decrease the PixArt- based LCM computation, leading to a reduction of  $10.98\%$  in MACs,  $24.33\%$  in parameter sizes, and  $6.02\%$  in latency with comparable generation results. We also visualize the generated samples based on different steps. As shown in Fig. 6, the difference caused by TGATE is invisible. These results demonstrate the effectiveness of TGATE.

Improvement over DeepCache. Table 7 compares DeepCache (Ma et al., 2023) and TGATE based on SDXL. Although DeepCache is more efficient, TGATE outperforms in generation quality. By reusing cross- attention maps and avoiding excessive caching, integrating TGATE with DeepCache yields superior results: 43.87T MACs and 14.67s in latency. Remarkably, DeepCache caches the mid- level blocks to decrease the computation, which is specific to the U- Net architecture. Its generalizability to other architectures, such as the transformer- based architecture, remains underexplored. Beyond DeepCache, TGATE has more wide applications and can considerably improve transformer- based diffusion models, as demonstrated in Table 4. This adaptability positions TGATE as a versatile and potent enhancement for various architectural frameworks.

Improvement over Different Schedulers. We test the generalizability of our approach on different noise schedulers. As shown in Table 8, we mainly consider three advanced schedulers (Karras et al., 2022; Lu et al., 2022; Song et al., 2020) that can compress the generation process of a transformer- based model to 25 inference steps. The experimental results show that our approach can consistently have stable generation performance in all settings, further indicating its potential for a broad application. Note that we attempt to use DDIM for PixArt, but the FID of PixArt w/o TGATE is larger than 100 and therefore we do not present this result in Table 8.

Table 8: Zero-shot FIDs from MS-COCO validation set using the base model of SDXL/PixArt and different noise schedulers (Karras et al., 2022; Lu et al., 2022; Song et al., 2020). The gate step is set as 10, and the total inference step is 25.  

<table><tr><td>Noise Scheduler</td><td>Base Model</td><td>+TGATE</td></tr><tr><td>EulerD (SDXL)</td><td>23.084</td><td>21.883</td></tr><tr><td>DDIM (SDXL)</td><td>21.377</td><td>21.783</td></tr><tr><td>DPMSolver (SDXL)</td><td>24.628</td><td>22.738</td></tr><tr><td>EulerD (PixArt)</td><td>39.008</td><td>37.587</td></tr><tr><td>DPMSolver (PixArt)</td><td>38.669</td><td>35.726</td></tr></table>

Table 9: Comparison with Adaptive Guidance (Castillo et al., 2023) on MS-COCO validation set based on SDXL and Pixart-Alpha. Params. refers to the number of parameters in the fidelity-improving stage.  

<table><tr><td>Inference Method</td><td>MACs</td><td>Params.</td><td>Latency</td><td>FID â</td></tr><tr><td>SDXL (w/ CFG)</td><td>149.438T</td><td>2.570B</td><td>53.187s</td><td>24.628</td></tr><tr><td>w/ Adaptive Guidance</td><td>104.606T</td><td>2.570B</td><td>35.538s</td><td>23.301</td></tr><tr><td>w/ TGATE (m=5)</td><td>84.438T</td><td rowspan="2">2.024B</td><td>27.932s</td><td>22.738</td></tr><tr><td>w/ TGATE (m=10)</td><td>100.688T</td><td>34.246s</td><td>23.433</td></tr><tr><td>PixelAlpha</td><td>107.031T</td><td>611.350M</td><td>61.502s</td><td>38.669</td></tr><tr><td>w/ Adaptive Guidance</td><td>74.922T</td><td>611.350M</td><td>42.684s</td><td>35.286</td></tr><tr><td>w/ TGATE (m=8)</td><td>65.318T</td><td rowspan="2">462.585M</td><td>37.867s</td><td>35.825</td></tr><tr><td>w/ TGATE (m=10)</td><td>70.225T</td><td>40.648s</td><td>35.726</td></tr></table>

Improvement over Adaptive Guidance. Adaptive guidance (Castillo et al., 2023) offers a strategy for the early termination of CFG. Our approach improves efficiency beyond adaptive guidance by innovatively caching and reusing cross- attention. Upon the termination of the CFG in PixArt- Alpha, adaptive guidance yields 2.14T MACs/step, whereas TGATE further reduces this to 1.83T MACs/step. This optimization allows a moderate reduction in computational overhead, as demonstrated in Table 9. Notably, recent trends have shifted towards distillation- based techniques (Song et al., 2023; Salimans & Ho, 2022) to accelerate inference. These methods compress the denoising process into fewer steps, often achieving single- digit iterations. In this distillation process, the student model learns to mimic the CFG- based output during training and, therefore, drops CFG during inference, rendering adaptive guidance inapplicable. On the contrary, TGATE can fill this gap and further accelerate the distillation- based models, as shown in Table 6. Beyond current capabilities, we also demonstrate superior scalability of TGATE compared to adaptive guidance, particularly as input sizes increase. This scalability feature of TGATE is further explored in next section.

Table 10: MACs per inference step when scaling up the token lengths and image resolutions.  

<table><tr><td rowspan="2">Resolution</td><td rowspan="2">Method</td><td colspan="4">Tokens Scaling Factor</td></tr><tr><td>Ã 1</td><td>Ã 128</td><td>Ã 1024</td><td>Ã 4096</td></tr><tr><td rowspan="2">768</td><td>SD-2.1 (w/o CFG)</td><td>0.761T</td><td>1.011T</td><td>2.774T</td><td>8.820T</td></tr><tr><td>Ours</td><td></td><td>0.73T</td><td></td><td></td></tr><tr><td rowspan="2">1024</td><td>SD-2.1 (w/o CFG)</td><td>1.351T</td><td>1.601T</td><td>3.364T</td><td>9.410T</td></tr><tr><td>Ours</td><td></td><td>1.298T</td><td></td><td></td></tr><tr><td rowspan="2">2048</td><td>SD-2.1 (w/o CFG)</td><td>5.398T</td><td>5.648T</td><td>7.411T</td><td>13.457T</td></tr><tr><td>Ours</td><td></td><td>5.191T</td><td></td><td></td></tr></table>

# 6.4. Discussion on Scaling Token Length and Resolution

Our method can improve efficiency by circumventing crossattention, motivating us to examine its contribution to the overall computational cost. This contribution depends on the input size. Specifically, we compare our method with SD- 2.1 w/o CFG per step to determine the computational cost of cross- attention. Note that SD- 2.1 w/o CFG is the computational lower bound for existing methods (Castillo et al., 2023) as it can stop CFG early to accelerate the diffusion process. As shown in Table 10, we use MACs as a measure of efficiency. In the default SD- 2.1 setting, the resolution is set as 768 with a maximum token length of 77. Our observations reveal that cross- attention constitutes a moderate share of total computation. Nonetheless, this share grows exponentially with increases in resolution and token lengths. By omitting cross- attention calculations, our approach significantly mitigates its adverse effects. For example, in an extreme scenario with the current architecture targeting an image size of 2048 and a token length of 4096  $\times 77$  we can decrease the MACs from 13.457T to 5.191T, achieving more than a twofold reduction in computation.

One may argue that existing models do not support such high resolutions or token lengths. However, there is an inevitable trend towards larger input sizes (Zhang et al., 2024; Esser et al., 2024; Chen et al., 2024). Furthermore, recent research (Li et al., 2024) indicates the difficulty in computing cross- attention on mobile devices, underscoring the practical benefits of our approach.

# 7. Conclusion

This paper has detailed the role of cross- attention in the inference process of text- conditional diffusion models. Our empirical analysis has led to several key insights: i) Crossattention converges within a few steps during inference. Upon its convergence, cross- attention only provides a minor impact on the denoising process. ii) By caching and reusing cross- attention after its convergence, our TGATE saves computation and improves FID scores. Our findings encourage the community to rethink cross- attention's role in text- to- image diffusion models.

# 8. Limitation

From the visualization results, the images created by TGATE are similar to those made without TGATE, with only minor differences in background. Although TGATE gets a better FID score, it might be difficult for users to spot the difference. Nevertheless, the main benefit of TGATE is the improvement of efficiency in the denoising process; The potential improvement in image quality is a spin- off.

# Acknowledgment

We thank Dylan R. Ashley, Bing Li, Haoqian Wu, Yuhui Wang, and Mingchen Zhuge for their valuable suggestions, discussions, and proofreading.

# References

Bau, D., Zhu, J.- Y., Strobelt, H., Lapedriza, A., Zhou, B., and Torralba, A. Understanding the role of individual units in a deep neural network. Proceedings of the National Academy of Sciences, 117(48):30071- 30078, 2020. Brooks, T., Peebles, B., Homes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., et al. Video generation models as world simulators, 2024. URL https://openai.com/research/videogeneration- models- as- world- simulators.Castillo, A., Kohler, J., PÃ©rez, J. C., PÃ©rez, J. P., Pumarola, A., Ghanem, B., ArbelÃ¡ez, P., and Thabet, A. Adaptive guidance: Training- free acceleration of conditional diffusion models. arXiv preprint arXiv:2312.12487, 2023. Chefer, H., Alaluf, Y., Vinker, Y., Wolf, L., and Cohen- Or, D. Attend- and- excite: Attention- based semantic guidance for text- to- image diffusion models. TOG, 42:148:1- 148:10, 2023. Chen, J., Yu, J., Ge, C., Yao, L., Xie, E., Wu, Y., Wang, Z., Kwok, J., Luo, P., Lu, H., and Li, Z. Pixart- Î±: Fast training of diffusion transformer for photorealistic text- to- image synthesis, 2023a.Chen, J., Ge, C., Xie, E., Wu, Y., Yao, L., Ren, X., Wang, Z., Luo, P., Lu, H., and Li, Z. Pixart-  $\backslash$  sigma: Weak- to- strong training of diffusion transformer for 4k text- to- image generation. arXiv preprint arXiv:2403.04692, 2024. Chen, S., Xu, M., Ren, J., Cong, Y., He, S., Xie, Y., Sinha, A., Luo, P., Xiang, T., and Perez- Rua, J.- M. Gentron: Delving deep into diffusion transformers for image and video generation. arXiv preprint arXiv:2312.04557, 2023b.Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. NeurIPS, 34:8780- 8794, 2021.

Esser, P., Kulal, S., Blattmann, A., Entezari, R., MÃ¼ller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high- resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. Fukushima, K. Neural network model for a mechanism of pattern recognition unaffected by shift in position- neocognitron. IEICE Technical Report, A, 62(10):658- 665, 1979. Fukushima, K. Neocognitron: A self- organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological cybernetics, 36(4):193- 202, 1980. He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 770- 778, 2016. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., and Cohen- Or, D. Prompt- to- prompt image editing with cross- attention control. In ICLR, 2023. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. Gans trained by a two time- scale update rule converge to a local nash equilibrium. NeurIPS, 30, 2017. Hinton, G., Vinyals, O., and Dean, J. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. Ho, J. and Salimans, T. Classifier- free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. NeurIPS, 33:6840- 6851, 2020. Hochreiter, S. Untersuchungen zu dynamischen neuronalen Netzen. Diploma thesis, Institut fÃ¼r Informatik, Lehrstuhl Prof. Brauer, Technische UniversitÃ¤t MÃ¼nchen, 1991. Advisor: J. Schmidhuber. Jarzynski, C. Equilibrium free- energy differences from nonequilibrium measurements: A master- equation approach. Physical Review E, 56(5):5018, 1997. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion- based generative models. Advances in Neural Information Processing Systems, 35: 26565- 26577, 2022. LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541- 551, 1989.

Li, Y., Wang, H., Jin, Q., Hu, J., Chemerys, P., Fu, Y., Wang, Y., Tulyakov, S., and Ren, J. Snapfusion: Text- to- image diffusion model on mobile devices within two seconds. Advances in Neural Information Processing Systems, 36, 2024. Lin, T.- Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., DollÃ¡r, P., and Zitnick, C. L. Microsoft coco: Common objects in context. In ECCV, pp. 740- 755. Springer, 2014. Liu, H., Zhuge, M., Li, B., Wang, Y., Faccio, F., Ghanem, B., and Schmidhuber, J. Learning to identify critical states for reinforcement learning from videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1955- 1965, 2023. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm- solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. Luo, S., Tan, Y., Huang, L., Li, J., and Zhao, H. Latent consistency models: Synthesizing high- resolution images with few- step inference. arXiv preprint arXiv:2310.04378, 2023. Ma, X., Fang, G., and Wang, X. Deepcache: Accelerating diffusion models for free. arXiv preprint arXiv:2312.00858, 2023. Neal, R. M. Annealed importance sampling. Statistics and computing, 11:125- 139, 2001. Pan, Z., Zhuang, B., Huang, D.- A., Nie, W., Yu, Z., Xiao, C., Cai, J., and Anandkumar, A. T- stitch: Accelerating sampling in pre- trained diffusion models with trajectory stitching. arXiv preprint arXiv:2402.14167, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 4195- 4205, 2023. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., MÃ¼ller, J., Penna, J., and Ronbach, R. Sdxl: Improving latent diffusion models for high- resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero- shot text- to- image generation. In ICML, pp. 8821- 8831. PMLR, 2021. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen, M. Hierarchical text- conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022.

Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High- resolution image synthesis with latent diffusion models. In CVPR, pp. 10684- 10695, 2022. Ronneberger, O., Fischer, P., and Brox, T. U- net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer- assisted intervention- MICCAI 2015: 18th international conference, Munich, Germany, October 5- 9, 2015, proceedings, part III 18, pp. 234- 241. Springer, 2015. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E. L., Ghasemipour, K., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al. Photorealistic text- to- image diffusion models with deep language understanding. NeurIPS, 35:36479- 36494, 2022. Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Schlag, I., Irie, K., and Schmidhuber, J. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pp. 9355- 9366. PMLR, 2021. Schmidhuber, J. Learning to control fast- weight memories: An alternative to recurrent nets. Neural Computation, 4 (1):131- 139, 1992a. Schmidhuber, J. Learning complex, extended sequences using the principle of history compression. Neural Computation, 4(2):234- 242, 1992b. (Based on TR FKI- 148- 91, TUM, 1991). Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Song, Y., Dhariwal, P., Chen, M., and Sutskever, I. Consistency models. 2023. Srivastava, R. K., Greff, K., and Schmidhuber, J. Highway networks. arXiv preprint arXiv:1505.00387, 2015. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wimbauer, F., Wu, B., Schoenfeld, E., Dai, X., Hou, J., He, Z., Sanakoyeu, A., Zhang, P., Tsai, S., Kohler, J., et al. Cache me if you can: Accelerating diffusion models through block caching. arXiv preprint arXiv:2312.03209, 2023. xiaoju ye. calflops: a flops and params calculate tool for neural networks in pytorch framework, 2023. URL https://github.com/MrYxJ/calculate- flops.pytorch.

Xie, J., Li, Y., Huang, Y., Liu, H., Zhang, W., Zheng, Y., and Shou, M. Z. Boxdiff: Text- to- image synthesis with training- free box- constrained diffusion. In ICCV, pp. 7452- 7461, 2023. Yang, X., Zhou, D., Feng, J., and Wang, X. Diffusion probabilistic model made slim. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22552- 22562, 2023. Zhang, B., Zhang, P., Dong, X., Zang, Y., and Wang, J. Long- clip: Unlocking the long- text capability of clip. arXiv preprint arXiv:2403.15378, 2024. Zhang, W., Tanida, J., Itoh, K., and Ichioka, Y. Shift- invariant pattern recognition neural network and its optical architecture. In Proceedings of annual conference of the Japan Society of Applied Physics, volume 564. Montreal, CA, 1988.